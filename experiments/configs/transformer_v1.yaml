# Transformer V1 Configuration
# Production model - longer training, high performance

model:
  type: transformer
  d_model: 256
  nhead: 8
  num_encoder_layers: 6
  num_decoder_layers: 6
  dim_feedforward: 1024
  dropout: 0.1
  max_seq_length: 1024

training:
  epochs: 100
  batch_size: 32
  learning_rate: 0.0001
  weight_decay: 0.00001
  gradient_clip: 1.0
  early_stopping_patience: 15
  mixed_precision: true  # Enable FP16 for faster training

data:
  max_seq_length: 1024
  num_workers: 4

optimizer:
  type: adamw
  betas: [0.9, 0.999]
  eps: 0.00000001

scheduler:
  type: reduce_on_plateau
  mode: min
  factor: 0.5
  patience: 5
  min_lr: 0.000001

# Expected Performance:
# - Training time: ~2-4 hours on GPU (500 pages)
# - CER: 8-15%
# - Character accuracy: 85-92%
# - Use case: Production deployment

# Transformer Large Configuration
# Research-grade model - best performance

model:
  type: transformer
  d_model: 512
  nhead: 16
  num_encoder_layers: 8
  num_decoder_layers: 8
  dim_feedforward: 2048
  dropout: 0.1
  max_seq_length: 1024

training:
  epochs: 150
  batch_size: 16  # Smaller batch due to model size
  learning_rate: 0.00005
  weight_decay: 0.00001
  gradient_clip: 1.0
  early_stopping_patience: 20
  mixed_precision: true

data:
  max_seq_length: 1024
  num_workers: 4

optimizer:
  type: adamw
  betas: [0.9, 0.999]
  eps: 0.00000001

scheduler:
  type: reduce_on_plateau
  mode: min
  factor: 0.5
  patience: 7
  min_lr: 0.000001

# Expected Performance:
# - Training time: ~6-8 hours on GPU (1000 pages)
# - CER: 2-5%
# - Character accuracy: 95-98%
# - Use case: State-of-the-art research, when highest quality needed
# - Requires: 16GB+ GPU memory

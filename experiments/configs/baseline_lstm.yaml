# LSTM Baseline Configuration
# Proof of concept - fast training, moderate performance

model:
  type: lstm
  embed_dim: 128
  encoder_hidden_dim: 256
  decoder_hidden_dim: 256
  num_layers: 2
  dropout: 0.3

training:
  epochs: 50
  batch_size: 64
  learning_rate: 0.001
  weight_decay: 0.00001
  gradient_clip: 1.0
  early_stopping_patience: 10
  mixed_precision: false

data:
  max_seq_length: 1024
  num_workers: 4

optimizer:
  type: adamw
  betas: [0.9, 0.999]

scheduler:
  type: reduce_on_plateau
  factor: 0.5
  patience: 5

# Expected Performance:
# - Training time: ~30 min on GPU (100 pages)
# - CER: 20-40%
# - Character accuracy: 60-80%
# - Use case: Proof of concept, baseline comparison
